# -*- coding: utf-8 -*-
"""4A_DIOQUINO_Exer2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l5OTLNJD02ddh_pPo04C5Jw2YvmnOX4c
"""

# Commented out IPython magic to ensure Python compatibility.
# Update package lists
!apt-get update

# Install necessary packages
!apt-get install -y cmake build-essential pkg-config

# Clone the OpenCV repository
!git clone https://github.com/opencv/opencv.git

# Clone the OpenCV contrib repository
!git clone https://github.com/opencv/opencv_contrib.git

# Create a build directory
!mkdir -p opencv/build

# Change to the build directory
# %cd opencv/build

# Run cmake with the necessary options
!cmake -D CMAKE_BUILD_TYPE=RELEASE \
-D CMAKE_INSTALL_PREFIX=/usr/local \
-D OPENCV_ENABLE_NONFREE=ON \
-D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules \
-D BUILD_EXAMPLES=OFF ..

# Compile OpenCV using all available cores (adjust the number as needed)
!make -j$(nproc)

# Install OpenCV
!make install

"""#Task 1 SIFT Feature Extraction"""

import cv2
import matplotlib.pyplot as plt

# For Google Colab: Upload the image if it's not already in the specified path
try:
    from google.colab import files
except ImportError:
    files = None

# Set the default image path
image_path = '/content/Diether,Dioquino A.jpg'

# Load the image or prompt for upload if not found
image = cv2.imread(image_path)
if image is None and files:
    print("Image not found. Please upload 'Diether,Dioquino A.jpg' or the correct file.")
    uploaded = files.upload()  # Allows you to upload the image in Colab

    # Get the uploaded file path
    image_path = next(iter(uploaded))  # Get the filename of the uploaded file
    image = cv2.imread(image_path)     # Load the image from the new path

# Check if the image was loaded successfully
if image is None:
    raise ValueError("Image not found. Please check the path or upload the correct file.")

# Convert the image to grayscale
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Initialize SURF detector (or use ORB as an alternative)
try:
    surf = cv2.xfeatures2d.SURF_create()
except AttributeError:
    print("SURF not available. Switching to ORB.")
    surf = cv2.ORB_create()  # Use ORB if SURF is unavailable

# Detect keypoints and descriptors
keypoints, descriptors = surf.detectAndCompute(gray_image, None)

# Draw keypoints on the image
image_with_keypoints = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

# Display the image with keypoints
plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))
plt.title('SURF Keypoints' if 'SURF' in str(surf) else 'ORB Keypoints')
plt.axis('off')  # Hide axis ticks
plt.show()

"""#Task 2 Surf Feature Extraction"""

import cv2
import matplotlib.pyplot as plt

# Load the image
image = cv2.imread('/content/img.jpg')
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Initialize SURF detector (requires OpenCV contrib package with non-free enabled)
surf = cv2.xfeatures2d.SURF_create()

# Detect keypoints and descriptors
keypoints, descriptors = surf.detectAndCompute(gray_image, None)

# Draw keypoints on the image
image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)

# Display the image with keypoints
plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))
plt.title('SURF Keypoints')
plt.show()

"""#Task 3 ORB Feature Extraction"""

import cv2
import matplotlib.pyplot as plt

# Check if running in Google Colab and import file upload function if available
try:
    from google.colab import files
except ImportError:
    files = None

# Specify the path to the image
image_path = '/content/img.jpg'

# Attempt to load the image
image = cv2.imread(image_path)
if image is None and files:
    print("Image not found. Please upload 'img.jpg'.")
    uploaded = files.upload()  # Prompts user to upload the image in Colab
    image_path = next(iter(uploaded))  # Update path to the uploaded filename
    image = cv2.imread(image_path)

# Confirm that the image loaded successfully
if image is None:
    raise ValueError("Image not found. Please check the path or upload the correct file.")

# Convert the image to grayscale
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Initialize the ORB detector
orb = cv2.ORB_create()

# Detect keypoints and descriptors
keypoints, descriptors = orb.detectAndCompute(gray_image, None)

# Draw keypoints on the image
image_with_keypoints = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

# Display the result
plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))
plt.title("ORB Keypoints")
plt.axis('off')  # Hide axis ticks
plt.show()

"""#Task 4 Feature Matching"""

# Load two images
image1 = cv2.imread('/content/Dioquino, Diether A.jpg', 0)
image2 = cv2.imread('/content/Dioquino.jpg', 0)

# Initialize SIFT detector
sift = cv2.SIFT_create()

# Find keypoints and descriptors with SIFT
keypoints1, descriptors1 = sift.detectAndCompute(image1, None)
keypoints2, descriptors2 = sift.detectAndCompute(image2, None)

# Initialize the matcher
bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)

# Match descriptors
matches = bf.match(descriptors1, descriptors2)

# Sort matches by distance (best matches first)
matches = sorted(matches, key=lambda x: x.distance)

# Draw matches
image_matches = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

# Display the matches
plt.imshow(image_matches)
plt.title('Feature Matching with SIFT')
plt.show()

"""#Task 5 Applications of Feature Matching"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load two images
image1 = cv2.imread('/content/Dioquino, Diether A.jpg', cv2.IMREAD_GRAYSCALE)
image2 = cv2.imread('/content/Dioquino.jpg', cv2.IMREAD_GRAYSCALE)

# Detect keypoints and descriptors using SIFT
sift = cv2.SIFT_create()
keypoints1, descriptors1 = sift.detectAndCompute(image1, None)
keypoints2, descriptors2 = sift.detectAndCompute(image2, None)

# Match features using BFMatcher
bf = cv2.BFMatcher(cv2.NORM_L2)
matches = bf.knnMatch(descriptors1, descriptors2, k=2)

# Apply ratio test (Lowe's ratio test)
good_matches = []
for m, n in matches:
    if m.distance < 0.75 * n.distance:
        good_matches.append(m)

# Extract location of good matches
src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

# Find homography matrix
M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)

# Check if a valid homography matrix was found
if M is not None:
    # Warp one image to align with the other
    h, w = image1.shape
    result = cv2.warpPerspective(image1, M, (w, h))

    # Display the result
    plt.imshow(result, cmap='gray')
    plt.title('Image Alignment using Homography')
    plt.axis('off')  # Hide axis ticks
    plt.show()
else:
    print("Homography matrix could not be found.")

"""#Combining Feature Extraction Methods"""

import cv2

# Use SIFT and ORB to extract features from two images

# Load two images
# Ensure correct paths to images
image1 = cv2.imread('/content/Dioquino, Diether A.jpg', 0)
image2 = cv2.imread('/content/Dioquino.jpg', 0)

# SIFT detector
sift = cv2.SIFT_create()
keypoints1_sift, descriptors1_sift = sift.detectAndCompute(image1, None)
keypoints2_sift, descriptors2_sift = sift.detectAndCompute(image2, None)

# ORB detector
orb = cv2.ORB_create()
keypoints1_orb, descriptors1_orb = orb.detectAndCompute(image1, None)
keypoints2_orb, descriptors2_orb = orb.detectAndCompute(image2, None)

"""#Processed Images"""

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import cv2
import numpy as np

# Assuming you have the following variables from your previous code:
# image_with_keypoints (SIFT), image_with_keypoints (SURF), image_with_keypoints (ORB), image_matches (feature matching)

# Load two images
image1 = cv2.imread('/content/Dioquino, Diether A.jpg', cv2.IMREAD_GRAYSCALE)
image2 = cv2.imread('/content/Dioquino.jpg', cv2.IMREAD_GRAYSCALE)

# Detect keypoints and descriptors using SIFT
sift = cv2.SIFT_create()
keypoints1, descriptors1 = sift.detectAndCompute(image1, None)
keypoints2, descriptors2 = sift.detectAndCompute(image2, None)

# Match features using BFMatcher
bf = cv2.BFMatcher(cv2.NORM_L2)
matches = bf.knnMatch(descriptors1, descriptors2, k=2)

# Apply ratio test (Lowe's ratio test)
good_matches = []
for m, n in matches:
    if m.distance < 0.75 * n.distance:
        good_matches.append(m)

# Extract location of good matches
src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

# Find homography matrix
M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)

# Check if a valid homography matrix was found
if M is not None:
    # Warp one image to align with the other
    h, w = image1.shape
    result = cv2.warpPerspective(image1, M, (w, h))

with PdfPages('processed_images.pdf') as pdf:
    plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))
    plt.title("SIFT Keypoints")
    pdf.savefig()
    plt.close()

    plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))  # Replace with your SURF image
    plt.title("SURF Keypoints")
    pdf.savefig()
    plt.close()

    plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))  # Replace with your ORB image
    plt.title("ORB Keypoints")
    pdf.savefig()
    plt.close()

    plt.imshow(image_matches)
    plt.title("Feature Matching with SIFT")
    pdf.savefig()
    plt.close()

    # ensure that result is defined
    if M is not None:
        plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))
        plt.title("Image Alignment using Homography")
        pdf.savefig()
        plt.close()


print("PDF saved as Exer2.pdf")